{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# classification Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**YOUR NAME, YOUR SURNAME**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600c92",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094e1c6",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| Python libraries contain previously combined set of codes that can be iteratively used, hence reducing time. As the term  suggests it’s similar to the physical library that holds reusable resources. However, in this instance it holds reusable lines of code that can be used in other programmes. In this project we used numpy, pandas, matplotlib, seaborn and sklearn libraries  for the purposes of data loading, manipulation, visualisation, preparation and model building. \n",
    "__1. Numpy:__\n",
    "Aids in mathematical ,numerical calculations and computations. It used in scientific, engineering, and data science programming. And is the most essential library for performing mathematical and statistical operations.\n",
    "__2. Pandas:__\n",
    "Provides us with vast facilities for data munipulation and analysis.\n",
    "__3. Matplotlib and seaborn:__\n",
    "These are both data visualisation libraries which provide functions to plot various data sets.\n",
    "__4. sklearn:__\n",
    "Is a software machine learning library which provides various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN. It is also designed to interoperate with numerical and scientific libraries such as, NumPy and SciPy|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623d1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5bff4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import comet_ml at the top of your file\n",
    "import comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3061253b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must import Comet before these modules: sklearn, keras, tensorboard, tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-54ea36717c74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create an experiment with your api key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m experiment = Experiment(\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mapi_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"uIA4LnUcw5MgXtaoZFKhjA8kp\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mproject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"team-14-classification\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\comet_ml\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, api_key, project_name, workspace, log_code, log_graph, auto_param_logging, auto_metric_logging, parse_args, auto_output_logging, log_env_details, log_git_metadata, log_git_patch, disabled, log_env_gpu, log_env_host, display_summary, log_env_cpu, display_summary_level, optimizer_data, auto_weight_logging, auto_log_co2, auto_metric_step_rate, auto_histogram_tensorboard_logging, auto_histogram_epoch_rate, auto_histogram_weight_logging, auto_histogram_gradient_logging, auto_histogram_activation_logging)\u001b[0m\n\u001b[0;32m    265\u001b[0m             )\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         super(Experiment, self).__init__(\n\u001b[0m\u001b[0;32m    268\u001b[0m             \u001b[0mproject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[0mworkspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\comet_ml\\experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, project_name, workspace, log_code, log_graph, auto_param_logging, auto_metric_logging, parse_args, auto_output_logging, log_env_details, log_git_metadata, log_git_patch, disabled, log_env_gpu, log_env_host, display_summary, log_env_cpu, display_summary_level, optimizer_data, auto_weight_logging, auto_log_co2, auto_metric_step_rate, auto_histogram_tensorboard_logging, auto_histogram_epoch_rate, auto_histogram_weight_logging, auto_histogram_gradient_logging, auto_histogram_activation_logging)\u001b[0m\n\u001b[0;32m    500\u001b[0m                     \u001b[0mALREADY_IMPORTED_MODULES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m                 )\n\u001b[1;32m--> 502\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;31m# Generate a unique identifier for this experiment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: You must import Comet before these modules: sklearn, keras, tensorboard, tensorflow"
     ]
    }
   ],
   "source": [
    "# import comet_ml at the top of your file\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"uIA4LnUcw5MgXtaoZFKhjA8kp\",\n",
    "    project_name=\"team-14-classification\",\n",
    "    workspace=\"bravo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475dbe93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9d6699a146af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m \u001b[1;31m# Import train_test_split function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;31m#Import scikit-learn metrics module for accuracy calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "# Libraries for data preparation and model building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "import keras\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a6718",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| Two main datasets will be used in this project.\n",
    "  **1. df_train_cvs loaded as df** \n",
    "  This dataframe will be used to train our model.\n",
    "  **2. df_test_cvs loaded as test_df**\n",
    "  test_df Will be used as our test data to assess the accurary of our model.|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbb6c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c1af387bc495>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_with_no_labels.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:\\Users\\Asus ROG\\Documents\\Notebooks\\Advanced Classification Sprint\\Predict\\Advanced_Classification_Predict-student_data-2780\\')\n",
    "test_df = pd.read_csv('test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81132ab3",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, to gain a better understanding of the data, we did a basic analysis of the dataframe to get an overview of the contents in the table. Moreover, we performed various statistical analysis methods and visualiations to gauge two main things. firstly, the relationship between the target variable(load_shortfall_3h) and the independent variables. Secondly, the relationship between the independent variables themselves. This analysis will aid us in determining which independant variables are relevant and weather or not we should remove or engineer new variables. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccfc644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25cd1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment     int64\n",
       "message      object\n",
       "tweetid       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the df datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65505f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "message      0\n",
       "tweetid      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the the df for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e805134e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:52:37.824204Z",
     "start_time": "2021-06-28T08:52:37.811206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15819.000000</td>\n",
       "      <td>15819.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.917504</td>\n",
       "      <td>501719.433656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.836537</td>\n",
       "      <td>289045.983132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>253207.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>502291.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>753769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>999888.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sentiment        tweetid\n",
       "count  15819.000000   15819.000000\n",
       "mean       0.917504  501719.433656\n",
       "std        0.836537  289045.983132\n",
       "min       -1.000000       6.000000\n",
       "25%        1.000000  253207.500000\n",
       "50%        1.000000  502291.000000\n",
       "75%        1.000000  753769.000000\n",
       "max        2.000000  999888.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the number summaries to try gather insight of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a004b4-9da7-48e4-82be-5c9f644a34ef",
   "metadata": {},
   "source": [
    "# Scatter Plots \n",
    "Here we plotted a few scatter plots for each city, showing the respective realtionships between the weather and load shortfall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884fc8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47961257-6c2c-4d1d-83b3-3c5cc0919bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa93ec6",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we will clean the data and create new features which will aid in preparing the data for modeling.|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d9124-80d2-4bbe-a6ed-244665a68655",
   "metadata": {},
   "source": [
    "# Cleaning The Data for both Training and Testing sets\n",
    "- We filled missing values of valencia pressure with the median\n",
    "- We converted and normalised the Data for Valencia_wind_deg and Seville_pressure\n",
    "- We converted time to its proper datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the training Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00ef27-7226-49d0-8e56-c75cdd620887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the Test Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51844238-a9ec-4a1a-8f6c-d04ae72b3480",
   "metadata": {},
   "source": [
    "# Gathered Insights\n",
    "- Time and day seem to be a big indicator of the load_shortfall so we will engineer new features to be used in our model.    \n",
    "- This graph also shows the High nature of the variance in the Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca73bf4-cf7f-4c93-9194-2b0cdc8597e2",
   "metadata": {},
   "source": [
    "# New Features\n",
    "- We created two new features called \"time of day\" and \"day\" which represent the the time of the day and the day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51344e2-d430-4df2-afe6-8bad28a05e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ae4ec-4ea5-40bb-92f5-bb45db8da900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af91780-e20d-451a-9df4-2fab81c17025",
   "metadata": {},
   "source": [
    "# Plotting New Feaures to veiw Relationships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965969a8-cfd6-42dc-a5b6-0d14459700da",
   "metadata": {},
   "source": [
    "# Gathered Insights\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849dc3e-4c67-4766-a8e3-cd1494e38753",
   "metadata": {},
   "source": [
    "# Relevant Features\n",
    "- We will use all the independent features except temperature averages to create our model, The reason for excluding \n",
    "temperature averages is due to multicollinearity with the other temperature features(min/max) in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2d523",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "|  To ensure we are getting the best possible predictions we will be using four regression models to compare the results. The overall idea of regression is to examine two things. Firstly, does a set of predictor variables do a good job in predicting an outcome (dependent) variable?. Secondly, which variables in particular are significant predictors of the outcome variable, and in what way do they–indicated by the magnitude and sign of the beta estimates–impact the outcome variable?  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables.\n",
    "  Description of models: \n",
    "   1. Ridge: Is a technique for analyzing multiple regression data that suffers from multicollinearity.\n",
    "   2. Linear: Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.\n",
    "   3. Random Forest: A Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap and Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.\n",
    "   4. Decision tree: builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bad69d-6d4e-4983-9d36-0fcb99a548bb",
   "metadata": {},
   "source": [
    "# Predictor vs Target Variables\n",
    "- nw_df was set to conatin all relevant predictor variables from feature engineering\n",
    "- Target was set to load_shortfall_3h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d0d473-8d1d-4cc4-811c-9daddad1c317",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-853ee9557361>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"message\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X =  df[\"message\"]\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1eadd37-7caa-4ec7-a27e-683896917873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Test Train split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c55c40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(max_features= 10546 ,ngram_range=(1, 2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0f4eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_X_train=tfidf.fit_transform(X_train)\n",
    "tf_X_test=tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a698ed4-0239-4bc3-a840-ecb634e0504a",
   "metadata": {},
   "source": [
    "# Selection of Models\n",
    "- We decided to use a few different types of models for our data set and would be judging accuracy based on a MSE and RMSE \n",
    "## Models Used\n",
    "- Linear - Simple Regression model \n",
    "- Ridge - Regression model using stadardisation and scaliing\n",
    "- Random Forest - Regression model using random forking variables\n",
    "- Decision Tree - Regression model using forking variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f201979-8bce-419d-8c0a-91505b85ecd3",
   "metadata": {},
   "source": [
    "### Training and Fitting the data on our selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d462bf8e-b103-4f36-ab80-6961c94bd359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the Support vector Machine Model\n",
    "sclf = svm.SVC(kernel='linear', C = 1.0)\n",
    "#clf=LinearSVC()\n",
    "sclf.fit(tf_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70b81751-723d-4d93-962a-177ee09c7986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the Random Forest Regression Model\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(tf_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b1d2738-c301-46ff-8763-2ebe89d4d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Decision Tree Regression Model\n",
    "dclf = DecisionTreeClassifier()\n",
    "dclf = dclf.fit(tf_X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c9a09-1a68-460a-b4ac-584522cb565f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c290b1-e86f-46b8-8dcb-8e11b84eca2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6793976f-520d-42a0-a6cc-df8f29952cbd",
   "metadata": {},
   "source": [
    "# Creating Predictions on our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b6d4fe4-7491-4136-ac9e-a038c5e4330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for testing data\n",
    "Random_pred=classifier .predict(tf_X_test)\n",
    "Svm_pred=clf .predict(tf_X_test)\n",
    "Decision_pred=dclf .predict(tf_X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b530251",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4f846-7b4f-475b-890c-fa1eb0adca8a",
   "metadata": {},
   "source": [
    "# Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4adee186-249d-467e-935b-d246d97b17d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.85      0.19      0.31       279\n",
      "           0       0.70      0.39      0.50       464\n",
      "           1       0.70      0.90      0.79      1691\n",
      "           2       0.77      0.72      0.74       730\n",
      "\n",
      "    accuracy                           0.72      3164\n",
      "   macro avg       0.76      0.55      0.59      3164\n",
      "weighted avg       0.73      0.72      0.69      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "print(classification_report(y_test,Random_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "67b09aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.78      0.42      0.54       279\n",
      "           0       0.70      0.46      0.55       464\n",
      "           1       0.77      0.90      0.83      1691\n",
      "           2       0.79      0.78      0.79       730\n",
      "\n",
      "    accuracy                           0.77      3164\n",
      "   macro avg       0.76      0.64      0.68      3164\n",
      "weighted avg       0.76      0.77      0.75      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#support vector machine\n",
    "print(classification_report(y_test,Svm_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "515aac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.47      0.37      0.41       279\n",
      "           0       0.42      0.46      0.44       464\n",
      "           1       0.72      0.73      0.73      1691\n",
      "           2       0.62      0.64      0.63       730\n",
      "\n",
      "    accuracy                           0.64      3164\n",
      "   macro avg       0.56      0.55      0.55      3164\n",
      "weighted avg       0.64      0.64      0.63      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision tree\n",
    "print(classification_report(y_test,Decision_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc210b-71a0-4bfd-8392-a3a42b2fe188",
   "metadata": {},
   "source": [
    "# Final Model Creation and CSV export\n",
    "- using our svm Model we created our prediction for the Test Data set with the following predictor columns\n",
    "- dropped time from our test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a9ad8add-8180-4dfa-aebd-58c6c384e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d5eaa12-5696-4bda-bf63-77f6cb2ece07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = tfidf.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99446541",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95796c3f-ccfd-4cf6-8aff-5162d5c1cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'tweetid': test_df.tweetid, 'sentiment': predictions})\n",
    "submission.to_csv('sebaka_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa811407-2131-4bd1-b7f6-f387c56e01a4",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "- Very low RMSE score of 3491, Random Forest was the best predictor of our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad0c0d",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a923e3",
   "metadata": {},
   "source": [
    "# comet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af39ea76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-270f6758bbe8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# log hash of your dataset to Comet.ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_dataset_hash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'experiment' is not defined"
     ]
    }
   ],
   "source": [
    "# log hash of your dataset to Comet.ml\n",
    "experiment.log_dataset_hash(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b491c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
